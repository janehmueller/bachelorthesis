% (3-4 Seiten)
\section{Named Entity Recognition and Linking}
\label{sec:NEL}
This section describes the classification problem combining both NER and NEL and its solution. Given an input text, all mentions of businesses should be recognized and linked to the correct business while other mentions of named entities should be ignored. Quote \ref{quote1} shows two example sentences about automobile businesses. Only the bold named entities are businesses and should be linked to entities in the knowledge base. The underlined entities are also named entities but not businesses. In this case, they are persons, but they could be organizations, like the EU, as well. Since the entities are used to extract relationships between businesses, the non-business entities must be filtered in some way. Finally, the business entities must be linked to entities in the knowledge base. To solve this problem, a classifier is trained using the German Wikipedia and its hand annotated links as knowledge base. This section first describes how the classifier, which solves the combined NER and EL problem, is trained and then explains documents are annotated.\\
\begin{nscenter}
	\fbox{\parbox[c][74pt]{\textwidth}{Sechs Top-Manager von \textbf{GM}, darunter \underline{Robert A. Lutz} sowie \underline{Carl-Peter Forster}, welcher als Group Vice President für das Europageschäft zuständig ist, trennten sich Anfang Mai 2009 von ihren gesamten Anteilsscheinen.\\PSA-Chef \underline{Carlos Tavares} hatte zugesagt, \textbf{Opel} als deutsches Unternehmen zu erhalten. Er hatte aber zugleich angekündigt, \textbf{Opel} müsse sich im Fall einer Übernahme durch \textbf{PSA} weitgehend aus eigener Kraft sanieren.}}
	\begin{quotecaption}
	Two example sentences containing highlighted named entities. Business entities are bold and other non-business entities are underlined.
	\label{quote1}
	\end{quotecaption}
\end{nscenter}

\subsection{Classifier Training}
The German Wikipedia has around 3.6 million pages and around 38 million links occurring in the text. Only a fraction of these contains relevant data, which is useful to train a classifier to link business entities. To reduce this massive amount of data to a more relevant dataset the links are filtered with the help of Wikidata. Wikidata is a structured knowledge base derived from Wikipedia. Using its ontology with the pages in Wikipedia can be reduced to only the pages about businesses. Let $W$ be the set of all Wikipedia pages and $W_{bsn} = \{ w \in W \FD w \text{ is tagged as business or a subclass of it in Wikidata} \}$. Let $E$ be the set of entities in the knowledge base and let function $f: E \to W$ map each entity in the knowledge base to its Wikipedia page so that $f(e) = w \AEQ w$ is the Wikipedia page about entity $e$.
\begin{definition}
A \textbf{link} l = (w,i,a,e) is the textual occurrence of an alias a in position i within Wikipedia page w and linking to the entity e.
\label{link}
\end{definition}
The elements of a tuple will be referred to via an index, i.e., given a link $l$, $l_w$ stands for the page $w$ on which $l$ occurs. Using the smaller set of business pages $W_{bsn}$, the relevant links can be reduced as well. Let $L$ be the set of all links occurring in Wikipedia and $L_{bsn} = \{ l \in L \FD f(l_e) \in W_{bsn} \}$. With this smaller set of links pointing to the pages of businesses, the possible aliases of businesses can be extracted. Let $A$ be the set of all aliases links in Wikipedia have and $A_{bsn} = \{ a \in A \FD \exists l \in L_{bsn}: l_a = a \}$. Next $L_{bsn}$ is expanded by selecting every link with an alias that has the possibility to link to a business. These links $M_{bsn} = \{ l \in L \FD \exists a \in A_{bsn}: a = l_a \}$ comprise the relevant links that will be used to train the classifier.\par
Due to the Wikipedia editing guidelines, however, there are two issues with these links: the mention of an entity on its own page is not linked to the page and usually only the first occurrence of an entity's mention is linked. To solve these two issues the links on a given Wikipedia page $w$ need to be extended. First, all links occurring on $w$ are extracted into the set $N_w = \{ l \in M_{bsn} \FD l_w = w\}$. These links are then used to create the set $O_w$, which contains the page $f(e)$ for every entity $e$ that a link on page $w$ links to. It also contains $w$ itself. Using $O_w$ all links linking to these pages are aggregated into $P_w = \{ l \in M_{bsn} \FD \exists w' \in O_w: f(l_e) = w' \}$.\par
Then $P_w$ is used to create the aliases $A_w = \{a \in A_{bsn} \FD \exists l \in P_w: l_a = a \}$. They are tokenized and these tokens are used to create a trie\ \cite{Fredkin:1960:TM:367390.367400}. In this trie each leaf represents a token of an alias. For example, the alias \itcite{Volkswagen AG} would be represented by the leaves \itcite{Volkswagen} and \itcite{AG}. With this trie all occurrences of the aliases $A_w$ on page $w$ are found and transformed into extended links $Q_w = \{ x \FD x_a \in A_w \AND \nexists l \in N_p: l_i = x_i \}$.\par
\begin{definition}
An \textbf{extended link} $x$ = (w,i,a,$E_a$) is the textual occurrence of an alias a in position i within Wikipedia page w. It possibly links to the entities $E_a$, which are defined as follows: $E_a = \{ e \in E \FD \exists l \in L: l_e = e \AND l_a = a \}$.
\label{extendedlink}
\end{definition}
These extended links are then transformed into actual links $R_w$ extending $N_w$ by applying a partial function $g$.
This function transforms an extended link $x = (w,i,a,E_a)$ into a link $l = (w,i,a,e)$ if and only if $E_a$ has only a single element $e$ or there is only one entity $e \in E_a$ for which there are at least two links with alias $a$ and these links make out at least 90\% of all links pointing to $e$. $S_w = R_w \cup N_w$ then makes up all the links on page $w$ used for the classifier training.\par
The links in $S_w$ all represent true annotations. But the classifier is also supposed to disambiguate true annotations and false, non-business ones, as seen in Quote \ref{quote1}. Thus the training set needs to be extended by false annotations. These are found by searching for the occurrences of known aliases on Wikipedia page $w$ which were not linked by a human. The assumption is that since these entities were not linked by a human that they are false in this context.
\begin{definition}
A \textbf{trie alias} t = (w,i,a) is the textual occurrence of an alias a in position i within Wikipedia page w found by using a trie of known aliases.
\label{triealias}
\end{definition}
These occurrences are found by creating a trie from the tokenized aliases in $A_{bsn}$ which is then used to find re-occurrences of known aliases. In the case of overlapping occurrences, only the longest occurrence is used. An example of such an overlap would be the sentence \itcite{Die Audi AG sitzt in Ingolstadt.}. Here the trie would find occurrences of both \itcite{Audi} and \itcite{Audi AG} but only the longest occurrence will be considered. These trie aliases make up the set
$T_w = \{ t = (w,i,a) \FD \nexists l \in S_w: l_i = t_i \}$ containing the false annotations on page $w$.\par
The number of trie aliases found is several times higher than the number of links and extended links per page. This can cause problems if there are so many more false annotations that the classifier just classifies every entry as wrong. It does this because there are so many false annotations that it maintains an accuracy of over 99\%. The number of trie aliases is, therefore, reduced by filtering stop words and symbols because they will almost never refer to businesses in newspaper articles.\par
To train the classifier the links and trie aliases of the small subset of Wikipedia pages $W_{bsn}$ are used. Every link in $S_w$ and every trie alias in $T_w$ is transformed into a number of feature entries. The possible pages a given alias $a$ can point to are $W_a = \{ w \in W \FD \exists l \in L_{bsn}: l_a = a \AND f(l_e) = w \}$.
\begin{definition}
A \textbf{feature entry} $fe$ = (w,i,a,e,F) represents an alias occurring on Wikipedia page w in position i and possibly pointing to an entity e with a set of generated features F used to train a classifier.
\label{featureentry}
\end{definition}
This set of features $F$ contains three first order features and six higher order features as described by Janetzki\ \cite{janetzki}. The higher order features are sometimes called second order features as well. The first order features are the link score, the entity score and the context score. Given a feature entry $fe = (w,i,a,e,F)$, the link score $f_1 \in F$ signifies how likely it is, that the alias $fe_a$ links to any entity. Therefore, every feature entry generated from the same link or trie alias has the same link score $f_1$. The entity score $f_2 \in F$ represents the likelihood that, given the alias $fe_a$ links to any entity, it links specifically to entity $fe_e$. The context score $f_3 \in F$ shows the similarity of the words around alias $fe_a$ on page $fe_w$ in position $fe_i$ to the words on page $f(fe_e)$. The higher order features are only generated for the entity score and the context score because the link score is equal for all feature entries generated from one link or trie alias. They are as described by Grütze et al. the rank, $\Delta top$ and $\Delta succ$\ \cite{coheel}. They describe the relationship between the feature entries generated by a single link or trie alias. The rank is an integer ranking of the values of the feature, which is defined by the partial order $\geq$. This means that the highest value of the feature has the lowest and best rank of 1 while the lowest value has the worst rank of $n$, with $n$ being the number of feature entries generated for a specific link or trie alias. $\Delta top$ describes the difference between the current feature entry to the one with rank 1. In the case of the feature entry with rank 1, its value is $\infty$. $\Delta succ$ describes the difference to the feature entry with the next higher (worse) rank. In the case of the worst feature entry with rank $n$ its value is also $\infty$.\par
Around 545 million feature entries containing these features were used to train the classifier. Different classification models will be evaluated in Section \ref{sec:ModelEval} and the first and higher order features will be evaluated in Section \ref{sec:FeatureEval}.

\subsection{Annotation of Newspaper Articles}
The process of annotating newspaper articles is very similar to the training process. Given an article $D$, the first task is to find possible mentions of businesses. This is done with the trie built using $A_{bsn}$. These trie aliases are again filtered by removing stop words and symbols since they are assumed to almost never reference a business. As just described, a number of feature entries are then generated from every trie alias found in article $D$. They are then classified using the trained classification model. Because one trie alias generates multiple feature entries, which are classified independently, it is possible that multiple feature entries generated from one trie alias are classified as links to businesses. When this happens, there is no way to rank the multiple possible entities since the classifier outputs only a boolean decision. Because of that, these collisions are filtered and the alias is not linked to any entity.
