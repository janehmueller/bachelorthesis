% (1 Seite)
\section{NEL on Newspaper Articles}
\label{sec:NELEval}
This section discusses the impact of different training data on and the performance of the presented NER and EL approach when annotating German newspaper articles. The documents were annotated on an eight node cluster using Apache Spark. Each node has a 6-Core CPU with 2.4 GHz and 64GB of RAM.\par

\subsection*{Impact of different Training Sets}
When the classifier is trained using only feature entries generated by links the resulting annotations contain a lot of false annotations. The inclusion of the trie aliases makes the classifier a lot more cautious by providing many occasions, where the alias is not linked at all. But this also adds many more feature entries and thereby increases the duration needed to train the model. It also adds so many negative entries that even Decision Tree models classify every feature entry as negative. The training set used in Section \ref{sec:ModelEval} and \ref{sec:FeatureEval} contains a total of 1.8 million positive feature entries and 545.9 million negative feature entries. When removing all feature entries with a rank $\geq 10$, the around 548 million feature entries are reduced to only 25.3 million. To preserve realistic precision and recall only the training data is filtered. Mainly negative feature entries are removed this way, which, therefore, fixes the problem of only negative predictions.\par
The selection of the Wikipedia pages used to generate the training data also influences the quality of the classifier. The precision and recall are not affected that strongly, but the annotation results vary greatly. Using all Wikipedia pages of businesses, which are around $1.5\%$ of all pages, results in better annotation results than using a random $1\%$ sample of pages. This is caused by the pages of businesses having, on average, more links than a random page and generally linking more to other businesses.\par

\subsection*{Performance}
The annotation of newspaper articles contains two steps. The first step is the generation of feature entries and the second the classification of those. The generation of the feature entries costs most of the time while the classification performs extremely well. The annotation of 3.6 million Wikipedia pages took 27 hours. As a comparison, the classification of 126 million feature entries, generated from $1\%$ of Wikipedia pages, took only 2 minutes. The performance of the feature generation could be improved in the future by storing the documents in a more storage efficient way. An example would be using a global word dictionary for all words and storing the indices for each word in a document. The current performance of the document annotation is still good, and it is possible, as shown, to annotate millions of documents in an acceptable amount of time. One big advantage of this performance is the possibility to, e.g., annotate newly published newspaper articles daily. This would regularly enrich the knowledge base of businesses with new relations and thus keep it up-to-date.\par
